{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0Augusto/Topicos_ComputacaoIII/blob/main/AS01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyG45Qk3qQLS"
      },
      "source": [
        "# Cells\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR921S_OQSHG"
      },
      "source": [
        "## Code cells\n",
        "Below is a **code cell**. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Cmd/Ctrl+Enter** to run the cell in place;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists); or\n",
        "* Type **Alt+Enter** to run the cell and insert a new code cell immediately below it.\n",
        "\n",
        "There are additional options for running some or all cells in the **Runtime** menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "both",
        "id": "WUtu4316QSHL"
      },
      "outputs": [],
      "source": [
        "#Normalizacao\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Reduzir o texto para letras minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remover acentos e diacríticos\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
        "\n",
        "    # Canonicalizar acrônimos, símbolos de moeda, datas e palavras hifenizadas\n",
        "    text = re.sub(r'\\b(\\w+)\\-(\\w+)\\b', r'\\1\\2', text)  # Remover hífens em palavras hifenizadas\n",
        "    text = re.sub(r'\\b(\\d{1,2})/(\\d{1,2})/(\\d{2,4})\\b', r'\\1-\\2-\\3', text)  # Formatar datas\n",
        "    text = re.sub(r'\\$', 'USD', text)  # Canonicalizar símbolo de moeda\n",
        "\n",
        "    # Remover pontuação (exceto para símbolos de moeda e datas)\n",
        "    text = re.sub(r'[^\\w\\s\\.\\-\\$]', '', text)\n",
        "\n",
        "    # Remover caracteres especiais\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Carregar e normalizar o texto\n",
        "with open('/content/sample_data/Shakespeare.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "normalized_text = normalize_text(text)\n",
        "\n",
        "# Salvar o texto normalizado em um arquivo\n",
        "with open('/content/sample_data/Shakespeare_Normalized.txt', 'w') as file:\n",
        "    file.write(normalized_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenização\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, WordPunctTokenizer, TweetTokenizer\n",
        "from nltk.tokenize.mwe import MWETokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize as gensim_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Carregar o texto normalizado\n",
        "with open('/content/sample_data/Shakespeare_Normalized.txt', 'r') as file:\n",
        "    normalized_text = file.read()\n",
        "\n",
        "# 1. Tokenização por espaço em branco\n",
        "tokenized_text_01 = normalized_text.split()\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized01.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_01))\n",
        "\n",
        "# 2. Tokenização usando o Word Tokenizer do NLTK\n",
        "tokenized_text_02 = word_tokenize(normalized_text)\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized02.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_02))\n",
        "\n",
        "# 3. Tokenização usando o Treebank Word Tokenizer do NLTK\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "tokenized_text_03 = treebank_tokenizer.tokenize(normalized_text)\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized03.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_03))\n",
        "\n",
        "# 4. Tokenização usando o Word Punctuation Tokenizer do NLTK\n",
        "word_punct_tokenizer = WordPunctTokenizer()\n",
        "tokenized_text_04 = word_punct_tokenizer.tokenize(normalized_text)\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized04.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_04))\n",
        "\n",
        "# 5. Tokenização usando o Tweet Tokenizer do NLTK\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tokenized_text_05 = tweet_tokenizer.tokenize(normalized_text)\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized05.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_05))\n",
        "\n",
        "# 6. Tokenização usando o MWE Tokenizer (Tokenizador de múltiplas palavras) do NLTK\n",
        "mwe_tokenizer = MWETokenizer([('multi', 'word'), ('two', 'word')])\n",
        "tokenized_text_06 = mwe_tokenizer.tokenize(normalized_text.split())\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized06.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_06))\n",
        "\n",
        "# 7. Tokenização usando o TextBlob\n",
        "tokenized_text_07 = TextBlob(normalized_text).words\n",
        "with open('/mnt/data/Shakespeare_Normalized_Tokenized07.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_07))\n",
        "\n",
        "# 8. Tokenização usando o spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(normalized_text)\n",
        "tokenized_text_08 = [token.text for token in doc]\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized08.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_08))\n",
        "\n",
        "# 9. Tokenização usando o Gensim\n",
        "tokenized_text_09 = list(gensim_tokenize(normalized_text))\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized09.txt', 'w') as file:\n",
        "    file.write(' '.join(tokenized_text_09))\n",
        "\n",
        "# 10. Tokenização usando o Keras Tokenization\n",
        "keras_tokenizer = Tokenizer()\n",
        "keras_tokenizer.fit_on_texts([normalized_text])\n",
        "tokenized_text_10 = keras_tokenizer.texts_to_sequences([normalized_text])[0]\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized10.txt', 'w') as file:\n",
        "    file.write(' '.join(map(str, tokenized_text_10)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "_npoQMgTk28V",
        "outputId": "bcfc5187-ce69-4c22-bcc6-e0453412f16b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.preprocessing.text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6321bb6a11fe>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensim_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Carregar o texto normalizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remoção de Stop-words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Baixar as stopwords do NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Carregar o texto tokenizado da tarefa 02 (NLTK Word Tokenizer)\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized02.txt', 'r') as file:\n",
        "    tokenized_text = file.read().split()\n",
        "\n",
        "# Remover stop-words usando a lista do NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [word for word in tokenized_text if word.lower() not in stop_words]\n",
        "\n",
        "# Salvar o texto sem stop-words em um arquivo\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized_StopWord.txt', 'w') as file:\n",
        "    file.write(' '.join(filtered_text))\n"
      ],
      "metadata": {
        "id": "TyiajEWpk_oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lematização\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Baixar os recursos do WordNet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Carregar o texto sem stop-words\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized_StopWord.txt', 'r') as file:\n",
        "    filtered_text = file.read().split()\n",
        "\n",
        "# Lematizar o texto\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
        "\n",
        "# Salvar o texto lematizado em um arquivo\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'w') as file:\n",
        "    file.write(' '.join(lemmatized_text))\n"
      ],
      "metadata": {
        "id": "8gKiVS_NlFYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Carregar o texto lematizado\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r') as file:\n",
        "    lemmatized_text = file.read().split()\n",
        "\n",
        "# 1. Aplicar o Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "stemmed_text_01 = [porter_stemmer.stem(word) for word in lemmatized_text]\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'w') as file:\n",
        "    file.write(' '.join(stemmed_text_01))\n",
        "\n",
        "# 2. Aplicar o Snowball Stemmer\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "stemmed_text_02 = [snowball_stemmer.stem(word) for word in lemmatized_text]\n",
        "with open('/content/sample_data/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'w') as file:\n",
        "    file.write(' '.join(stemmed_text_02))\n"
      ],
      "metadata": {
        "id": "RyIzH7W-lMCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Análise do Vocabulário\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_vocabulary(text, output_file):\n",
        "    # Contar a frequência de cada token no texto\n",
        "    counter = Counter(text)\n",
        "    # Salvar a análise em um arquivo CSV\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['Token', 'Occurrences', 'Length']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for token, count in counter.items():\n",
        "            writer.writerow({'Token': token, 'Occurrences': count, 'Length': len(token)})\n",
        "\n",
        "# Análise do vocabulário para o texto lematizado\n",
        "analyze_vocabulary(lemmatized_text, '/content/sample_data/Shakespeare_Vocabulary_Lemmatized.csv')\n",
        "\n",
        "# Análise do vocabulário para o texto com Porter Stemmer\n",
        "analyze_vocabulary(stemmed_text_01, '/content/sample_data/Shakespeare_Vocabulary_Porter.csv')\n",
        "\n",
        "# Análise do vocabulário para o texto com Snowball Stemmer\n",
        "analyze_vocabulary(stemmed_text_02, '/content/sample_data/Shakespeare_Vocabulary_Snowball.csv')\n"
      ],
      "metadata": {
        "id": "0qpslqX-lS8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparação do Vocabulário\n",
        "def compare_vocabulary(lemmatized_file, porter_file, snowball_file, output_file):\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.write(\"Vocabulary Analysis\\n\\n\")\n",
        "        for method, filename in [(\"Lemmatizer\", lemmatized_file), (\"Porter Stemmer\", porter_file), (\"Snowball Stemmer\", snowball_file)]:\n",
        "            with open(filename, 'r') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "                tokens = [row['Token'] for row in reader]\n",
        "                avg_occurrences = sum(int(row['Occurrences']) for row in reader) / len(tokens)\n",
        "                avg_length = sum(int(row['Length']) for row in reader) / len(tokens)\n",
        "                file.write(f\"{method}:\\n\")\n",
        "                file.write(f\"Tamanho do Vocabulário: {len(tokens)}\\n\")\n",
        "                file.write(f\"Média de Ocorrências: {avg_occurrences:.2f}\\n\")\n",
        "                file.write(f\"Tamanho Médio dos Tokens: {avg_length:.2f}\\n\\n\")\n",
        "\n",
        "compare_vocabulary(\n",
        "    '/content/sample_data/Shakespeare_Vocabulary_Lemmatized.csv',\n",
        "    '/content/sample_data/Shakespeare_Vocabulary_Porter.csv',\n",
        "    '/content/sample_data/Shakespeare_Vocabulary_Snowball.csv',\n",
        "    '/content/sample_data/Shakespeare_Vocabulary_Analysis.txt'\n",
        ")\n"
      ],
      "metadata": {
        "id": "xFnIT619lZT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}